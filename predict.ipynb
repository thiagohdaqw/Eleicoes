{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dd5bd742-a154-4a55-a46f-a7ea3cafa129",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.pipeline import PipelineModel\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import lower, when, col, udf, split, lit, format_string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b891117c-0759-4a5d-94fc-189fe5bba57a",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b06223cf-9b18-4747-a4cb-0bac6216f341",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPARK_MASTER = \"spark://localhost:5000\"\n",
    "KAFKA_SERVER = 'localhost:9093'\n",
    "SPARK_APP_NAME = \"Final - PSPD - Predict\"\n",
    "INTERVAL = \"10 seconds\"\n",
    "\n",
    "PREDICT_TOPIC = 'predict'\n",
    "STATS_TOPIC = 'test-elasticsearch-sink'\n",
    "\n",
    "PACKAGES = \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.0\"\n",
    "\n",
    "MODEL_PATH = \"model/trained.model\"\n",
    "STOPWORDS_PATH = \"dataset/stopwords.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e16a5f-aacf-47e0-a96a-60bd32f65b01",
   "metadata": {},
   "source": [
    "## Startup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ecfb9ea-fbbf-41c3-876f-674535dfd199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/thiago/.local/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/thiago/.ivy2/cache\n",
      "The jars for the packages stored in: /home/thiago/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-8ddb0f41-291b-49ce-9a02-8f33792ba28c;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.2.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.2.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.8.0 in central\n",
      "\tfound org.lz4#lz4-java;1.7.1 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.4 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.1 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.1 in central\n",
      "\tfound org.apache.htrace#htrace-core4;4.1.0-incubating in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
      ":: resolution report :: resolve 588ms :: artifacts dl 18ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.1 from central in [default]\n",
      "\torg.apache.htrace#htrace-core4;4.1.0-incubating from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.8.0 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.2.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.2.0 from central in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.4 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   13  |   0   |   0   |   0   ||   13  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-8ddb0f41-291b-49ce-9a02-8f33792ba28c\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 13 already retrieved (0kB/9ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/18 16:10:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "conf = SparkConf() \\\n",
    "    .setMaster(SPARK_MASTER) \\\n",
    "    .setAppName(SPARK_APP_NAME) \\\n",
    "    .set(\"spark.jars.packages\", PACKAGES)\n",
    "    \n",
    "context = SparkContext(conf=conf)\n",
    "context.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46c7cda4-1a80-4596-ae4a-eb2709436582",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbbfaf5-8bed-4e2b-8944-9a9552096258",
   "metadata": {},
   "source": [
    "## Cleaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d5bd50a9-1d8f-466d-8d6f-f8a569b704be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "CLEAN_REGEX = r\"[.,/\\\\\\[\\]\\{\\}`~^\\d&!@#$%*\\)\\(\\'\\\"<>=+-:;?]\"\n",
    "\n",
    "stopwords = set()\n",
    "\n",
    "with open(STOPWORDS_PATH, \"r\") as stop_file:\n",
    "    for w in stop_file:\n",
    "        stopwords.add(w.strip().lower())\n",
    "\n",
    "def cleaner(sentence):\n",
    "    print(sentence)\n",
    "    sentence = \" \".join(\n",
    "        filter(\n",
    "            lambda x: x not in stopwords,\n",
    "            re.sub(CLEAN_REGEX, '', sentence).split()\n",
    "        )\n",
    "    )\n",
    "    return sentence\n",
    "\n",
    "cleaner_col = udf(lambda s: cleaner(s), StringType())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203786bf",
   "metadata": {},
   "source": [
    "## Load Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c56c630",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "model = PipelineModel.load(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47b59bd",
   "metadata": {},
   "source": [
    "## Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "37e20c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = spark.read.option(\"inferSchema\", \"true\").text(\"test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9a658a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+---------------+\n",
      "|               value|candidate|       sentence|\n",
      "+--------------------+---------+---------------+\n",
      "|lula,pessimo,asdadsa|     lula|pessimo,asdadsa|\n",
      "|  bolsonaro,horrivel|bolsonaro|       horrivel|\n",
      "|          lula,otimo|     lula|          otimo|\n",
      "+--------------------+---------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "candidateMessage = split(lines.value, \",\", 2)\n",
    "sentences = lines \\\n",
    "            .withColumn(\"candidate\", candidateMessage.getItem(0)) \\\n",
    "            .withColumn(\"sentence\", candidateMessage.getItem(1))\n",
    "sentences.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0446fa-8a4d-419d-a6d6-5955ca1c13e3",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6b31a2c1-70c5-42e8-889c-6dff59b78f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def foreach_batch_func(df: DataFrame, _):\n",
    "    # Preparations - split into candidate and message and clean\n",
    "    candidateMessage = split(df.value, \",\", 2)\n",
    "    sentences = df \\\n",
    "                .withColumn(\"candidate\", candidateMessage.getItem(0)) \\\n",
    "                .withColumn(\"sentence\", cleaner_col(lower(candidateMessage.getItem(1))))\n",
    "\n",
    "    # Predict\n",
    "    prediction = model.transform(sentences) \\\n",
    "                .select(\n",
    "                    \"candidate\",\n",
    "                    \"sentence\",\n",
    "                    \"probability\",\n",
    "                    when(col(\"prediction\") == 1.0, \"positive\").otherwise(\"negative\").alias(\"prediction\")\n",
    "                ) \\\n",
    "\n",
    "    # Write in console\n",
    "    prediction \\\n",
    "        .write \\\n",
    "        .format(\"console\") \\\n",
    "        .save()\n",
    "\n",
    "    # Prepare prediction to elasticsearch format\n",
    "    # Group by candidate and prediction and format to json\n",
    "    predictionElastic = prediction \\\n",
    "                        .groupBy(\n",
    "                            \"candidate\",\n",
    "                            \"prediction\"\n",
    "                        ).count() \\\n",
    "                        .select(\n",
    "                            lit('1').alias(\"key\"),\n",
    "                            format_string(\n",
    "                                \"{\\\"candidate\\\": \\\"%s\\\", \\\"%s\\\": %d}\",\n",
    "                                col(\"candidate\"), col(\"prediction\"), col(\"count\")\n",
    "                            ).alias(\"value\")\n",
    "                        )\n",
    "    \n",
    "    # Write to kafka elasticsearch topic\n",
    "    predictionElastic.write \\\n",
    "                    .format(\"kafka\") \\\n",
    "                    .option(\"kafka.bootstrap.servers\", KAFKA_SERVER) \\\n",
    "                    .option('topic', STATS_TOPIC) \\\n",
    "                    .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ee39d3-be33-4eec-b043-156b1c81a5be",
   "metadata": {},
   "source": [
    "## Sink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ec95c249-6eeb-49a5-ae56-53c6910e20d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+--------------------+----------+\n",
      "|candidate|sentence|         probability|prediction|\n",
      "+---------+--------+--------------------+----------+\n",
      "|     lula|  alegra|[0.05473215335437...|  positive|\n",
      "+---------+--------+--------------------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------------+--------------------+----------+\n",
      "|candidate|        sentence|         probability|prediction|\n",
      "+---------+----------------+--------------------+----------+\n",
      "|bolsonaro|odeio filho puta|[0.99998649826756...|  negative|\n",
      "+---------+----------------+--------------------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+--------------------+----------+\n",
      "|candidate|sentence|         probability|prediction|\n",
      "+---------+--------+--------------------+----------+\n",
      "|     lula|      la|[0.05473215335437...|  positive|\n",
      "|     lula|      la|[0.05473215335437...|  positive|\n",
      "+---------+--------+--------------------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+--------------------+----------+\n",
      "|candidate|sentence|         probability|prediction|\n",
      "+---------+--------+--------------------+----------+\n",
      "|     lula|      la|[0.05473215335437...|  positive|\n",
      "+---------+--------+--------------------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------------+--------------------+----------+\n",
      "|candidate|        sentence|         probability|prediction|\n",
      "+---------+----------------+--------------------+----------+\n",
      "|     lula|odeio filho puta|[0.99998649826756...|  negative|\n",
      "|bolsonaro|           fuder|[0.05473215335437...|  positive|\n",
      "+---------+----------------+--------------------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+--------------------+----------+\n",
      "|candidate|sentence|         probability|prediction|\n",
      "+---------+--------+--------------------+----------+\n",
      "|     lula|   fuder|[0.05473215335437...|  positive|\n",
      "+---------+--------+--------------------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+--------------------+----------+\n",
      "|candidate|sentence|         probability|prediction|\n",
      "+---------+--------+--------------------+----------+\n",
      "|bolsonaro|tomar cu|[0.99152905470935...|  negative|\n",
      "|     lula|amo voce|[0.05473215335437...|  positive|\n",
      "|bolsonaro|amo voce|[0.05473215335437...|  positive|\n",
      "+---------+--------+--------------------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+--------------------+----------+\n",
      "|candidate|            sentence|         probability|prediction|\n",
      "+---------+--------------------+--------------------+----------+\n",
      "|     lula|            tomar cu|[0.99152905470935...|  negative|\n",
      "|bolsonaro|            tomar cu|[0.99152905470935...|  negative|\n",
      "|bolsonaro|               feliz|[0.03131361789908...|  positive|\n",
      "|     lula|               feliz|[0.03131361789908...|  positive|\n",
      "|     lula|            tomar cu|[0.99152905470935...|  negative|\n",
      "|bolsonaro|            tomar cu|[0.99152905470935...|  negative|\n",
      "|bolsonaro|               feliz|[0.03131361789908...|  positive|\n",
      "|     lula|               feliz|[0.03131361789908...|  positive|\n",
      "|     lula|            tomar cu|[0.99152905470935...|  negative|\n",
      "|bolsonaro|            tomar cu|[0.99152905470935...|  negative|\n",
      "|bolsonaro|               feliz|[0.03131361789908...|  positive|\n",
      "|     lula|               feliz|[0.03131361789908...|  positive|\n",
      "|     lula|            tomar cu|[0.99152905470935...|  negative|\n",
      "|bolsonaro|            tomar cu|[0.99152905470935...|  negative|\n",
      "|bolsonaro|               feliz|[0.03131361789908...|  positive|\n",
      "|     lula|felizlulavai toma...|[0.99152905470935...|  negative|\n",
      "|bolsonaro|            tomar cu|[0.99152905470935...|  negative|\n",
      "|bolsonaro|               feliz|[0.03131361789908...|  positive|\n",
      "|     lula|felizlulavai toma...|[0.99152905470935...|  negative|\n",
      "|bolsonaro|            tomar cu|[0.99152905470935...|  negative|\n",
      "+---------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "lines = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_SERVER) \\\n",
    "    .option(\"subscribe\", PREDICT_TOPIC) \\\n",
    "    .option(\"failOnDataLoss\", \"false\") \\\n",
    "    .load() \\\n",
    "    .writeStream \\\n",
    "    .foreachBatch(foreach_batch_func) \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/spark/mllib-predict\") \\\n",
    "    .trigger(processingTime=INTERVAL) \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b62552-5398-48a7-ae97-852f095f7b87",
   "metadata": {},
   "source": [
    "# End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bc721391-1f82-4344-bd48-e672569b95fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a84f92-467a-4e3a-bf9f-f734226abc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "context.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
