{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd5bd742-a154-4a55-a46f-a7ea3cafa129",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.pipeline import PipelineModel\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import lower, when, col, udf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b891117c-0759-4a5d-94fc-189fe5bba57a",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b06223cf-9b18-4747-a4cb-0bac6216f341",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_FILE = \"model/trained.model\"\n",
    "SPARK_MASTER = \"spark://localhost:5000\"\n",
    "SPARK_APP_NAME = \"Final - PSPD - Predict\"\n",
    "KAFKA_SERVER = 'localhost:9093'\n",
    "PREDICT_TOPIC = 'predict'\n",
    "PACKAGES = \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e16a5f-aacf-47e0-a96a-60bd32f65b01",
   "metadata": {},
   "source": [
    "## Startup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ecfb9ea-fbbf-41c3-876f-674535dfd199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/thiago/.local/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/thiago/.ivy2/cache\n",
      "The jars for the packages stored in: /home/thiago/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-4dca2192-51a0-4b8d-9200-c51dda7b61bb;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.2.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.2.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.8.0 in central\n",
      "\tfound org.lz4#lz4-java;1.7.1 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.4 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.1 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.1 in central\n",
      "\tfound org.apache.htrace#htrace-core4;4.1.0-incubating in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
      ":: resolution report :: resolve 424ms :: artifacts dl 25ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.1 from central in [default]\n",
      "\torg.apache.htrace#htrace-core4;4.1.0-incubating from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.8.0 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.2.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.2.0 from central in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.4 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   13  |   0   |   0   |   0   ||   13  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-4dca2192-51a0-4b8d-9200-c51dda7b61bb\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 13 already retrieved (0kB/8ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/17 22:44:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "conf = SparkConf() \\\n",
    "    .setMaster(SPARK_MASTER) \\\n",
    "    .setAppName(SPARK_APP_NAME) \\\n",
    "    .set(\"spark.jars.packages\", PACKAGES)\n",
    "    \n",
    "context = SparkContext(conf=conf)\n",
    "context.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46c7cda4-1a80-4596-ae4a-eb2709436582",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbbfaf5-8bed-4e2b-8944-9a9552096258",
   "metadata": {},
   "source": [
    "## Cleaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5bd50a9-1d8f-466d-8d6f-f8a569b704be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "STOPWORDS_PATH = \"dataset/stopwords.txt\"\n",
    "CLEAN_REGEX = r\"[.,/\\\\\\[\\]\\{\\}`~^\\d&!@#$%*\\)\\(\\'\\\"<>=+-:;?]\"\n",
    "\n",
    "stopwords = set()\n",
    "\n",
    "with open(STOPWORDS_PATH, \"r\") as stop_file:\n",
    "    for w in stop_file:\n",
    "        stopwords.add(w.strip().lower())\n",
    "\n",
    "def cleaner(sentence):\n",
    "    sentence = \" \".join(\n",
    "        filter(\n",
    "            lambda x: x not in stopwords,\n",
    "            re.sub(CLEAN_REGEX, '', sentence).split()\n",
    "        )\n",
    "    )\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0446fa-8a4d-419d-a6d6-5955ca1c13e3",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b31a2c1-70c5-42e8-889c-6dff59b78f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaner_func = udf(lambda s: cleaner(s), StringType())\n",
    "def foreach_batch_func(df: DataFrame, _):\n",
    "    sentences = df.select(cleaner_func(lower(df.value)).alias(\"sentence\"))\n",
    "\n",
    "    model = PipelineModel.load(MODEL_FILE)\n",
    "    prediction = model.transform(sentences)\n",
    "\n",
    "    prediction \\\n",
    "        .select(\n",
    "            \"sentence\",\n",
    "            \"probability\",\n",
    "            when(col(\"prediction\") == 1.0, \"positive\").otherwise(\"negative\").alias(\"prediction\")\n",
    "        ) \\\n",
    "        .write \\\n",
    "        .format(\"console\") \\\n",
    "        .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ee39d3-be33-4eec-b043-156b1c81a5be",
   "metadata": {},
   "source": [
    "## Sink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec95c249-6eeb-49a5-ae56-53c6910e20d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+----------+\n",
      "|sentence|         probability|prediction|\n",
      "+--------+--------------------+----------+\n",
      "|   odeio|[0.27990843001391...|  positive|\n",
      "|horr√≠vel|[0.27990843001391...|  positive|\n",
      "|   feliz|[0.18525478159637...|  positive|\n",
      "|  amavel|[0.27990843001391...|  positive|\n",
      "+--------+--------------------+----------+\n",
      "\n",
      "+--------+--------------------+----------+\n",
      "|sentence|         probability|prediction|\n",
      "+--------+--------------------+----------+\n",
      "|   otimo|[0.27990843001391...|  positive|\n",
      "|        |[0.27990843001391...|  positive|\n",
      "|  triste|[0.27990843001391...|  positive|\n",
      "|  triste|[0.27990843001391...|  positive|\n",
      "|   feliz|[0.18525478159637...|  positive|\n",
      "+--------+--------------------+----------+\n",
      "\n",
      "+--------+--------------------+----------+\n",
      "|sentence|         probability|prediction|\n",
      "+--------+--------------------+----------+\n",
      "|  triste|[0.27990843001391...|  positive|\n",
      "+--------+--------------------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+----------+\n",
      "|sentence|         probability|prediction|\n",
      "+--------+--------------------+----------+\n",
      "|horrivel|[0.27990843001391...|  positive|\n",
      "+--------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lines = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_SERVER) \\\n",
    "    .option(\"subscribe\", PREDICT_TOPIC) \\\n",
    "    .option(\"failOnDataLoss\", \"false\") \\\n",
    "    .load() \\\n",
    "    .writeStream \\\n",
    "    .foreachBatch(foreach_batch_func) \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/spark/mllib-predict\") \\\n",
    "    .trigger(processingTime=\"10 seconds\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b62552-5398-48a7-ae97-852f095f7b87",
   "metadata": {},
   "source": [
    "# End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc721391-1f82-4344-bd48-e672569b95fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines.stop()\n",
    "spark.stop()\n",
    "context.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a84f92-467a-4e3a-bf9f-f734226abc50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
