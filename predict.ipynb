{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5bd742-a154-4a55-a46f-a7ea3cafa129",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.pipeline import PipelineModel\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import lower, when, col, udf, split, lit, format_string\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b891117c-0759-4a5d-94fc-189fe5bba57a",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06223cf-9b18-4747-a4cb-0bac6216f341",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_FILE = os.getenv(\"TRAINING_FILE\",\"dataset/dataset.csv\")\n",
    "SPARK_MASTER = os.getenv(\"SPARK_MASTER\", \"spark://gpu3.esw:7077\")\n",
    "KAFKA_SERVER = os.getenv(\"KAFKA_SERVER\", 'localhost:9092')\n",
    "\n",
    "SPARK_APP_NAME = \"Final - PSPD - Predict\"\n",
    "INTERVAL = os.getenv(\"INTERVAL\", \"10 seconds\")\n",
    "\n",
    "PREDICT_TOPIC = os.getenv(\"PREDICT_TOPIC\", 'election')\n",
    "STATS_TOPIC = os.getenv(\"STATS_TOPIC\", 'elasticsearch-sink')\n",
    "\n",
    "PACKAGES = \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.0\"\n",
    "\n",
    "PRETRAINED_MODEL_PATH = os.getenv(\"PRETRAINED_MODEL_PATH\", \"model/trained.model\")\n",
    "STOPWORDS_PATH = os.getenv(\"STOPWORDS_PATH\", \"dataset/stopwords.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e16a5f-aacf-47e0-a96a-60bd32f65b01",
   "metadata": {},
   "source": [
    "## Startup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecfb9ea-fbbf-41c3-876f-674535dfd199",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf() \\\n",
    "    .setMaster(SPARK_MASTER) \\\n",
    "    .setAppName(SPARK_APP_NAME) \\\n",
    "    .set(\"spark.jars.packages\", PACKAGES)\n",
    "    \n",
    "context = SparkContext(conf=conf)\n",
    "context.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c7cda4-1a80-4596-ae4a-eb2709436582",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbbfaf5-8bed-4e2b-8944-9a9552096258",
   "metadata": {},
   "source": [
    "## Cleaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bd50a9-1d8f-466d-8d6f-f8a569b704be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "CLEAN_REGEX = r\"[.,/\\\\\\[\\]\\{\\}`~^\\d&!@#$%*\\)\\(\\'\\\"<>=+-:;?]\"\n",
    "\n",
    "stopwords = set()\n",
    "\n",
    "with open(STOPWORDS_PATH, \"r\") as stop_file:\n",
    "    for w in stop_file:\n",
    "        stopwords.add(w.strip().lower())\n",
    "\n",
    "def cleaner(sentence):\n",
    "    print(sentence)\n",
    "    sentence = \" \".join(\n",
    "        filter(\n",
    "            lambda x: x not in stopwords,\n",
    "            re.sub(CLEAN_REGEX, '', sentence).split()\n",
    "        )\n",
    "    )\n",
    "    return sentence\n",
    "\n",
    "cleaner_col = udf(lambda s: cleaner(s), StringType())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203786bf",
   "metadata": {},
   "source": [
    "## Load Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c56c630",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PipelineModel.load(PRETRAINED_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0446fa-8a4d-419d-a6d6-5955ca1c13e3",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b31a2c1-70c5-42e8-889c-6dff59b78f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def foreach_batch_func(df: DataFrame, _):\n",
    "    # Preparations - split into candidate and message and clean\n",
    "    candidateMessage = split(df.value, \",\", 2)\n",
    "    sentences = df \\\n",
    "                .withColumn(\"candidate\", candidateMessage.getItem(0)) \\\n",
    "                .withColumn(\"sentence\", cleaner_col(lower(candidateMessage.getItem(1))))\n",
    "\n",
    "    # Predict\n",
    "    prediction = model.transform(sentences) \\\n",
    "                .select(\n",
    "                    \"candidate\",\n",
    "                    \"sentence\",\n",
    "                    \"probability\",\n",
    "                    when(col(\"prediction\") == 1.0, \"positive\").otherwise(\"negative\").alias(\"prediction\")\n",
    "                ) \\\n",
    "\n",
    "    # Write in console\n",
    "    prediction \\\n",
    "        .write \\\n",
    "        .format(\"console\") \\\n",
    "        .save()\n",
    "\n",
    "    # Prepare prediction to elasticsearch format\n",
    "    # Group by candidate and prediction and format to json\n",
    "    predictionElastic = prediction \\\n",
    "                        .groupBy(\n",
    "                            \"candidate\",\n",
    "                            \"prediction\"\n",
    "                        ).count() \\\n",
    "                        .select(\n",
    "                            lit('1').alias(\"key\"),\n",
    "                            format_string(\n",
    "                                \"{\\\"candidate\\\": \\\"%s\\\", \\\"%s\\\": %d}\",\n",
    "                                col(\"candidate\"), col(\"prediction\"), col(\"count\")\n",
    "                            ).alias(\"value\")\n",
    "                        )\n",
    "    \n",
    "    # Write to kafka elasticsearch topic\n",
    "    predictionElastic.write \\\n",
    "                    .format(\"kafka\") \\\n",
    "                    .option(\"kafka.bootstrap.servers\", KAFKA_SERVER) \\\n",
    "                    .option('topic', STATS_TOPIC) \\\n",
    "                    .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ee39d3-be33-4eec-b043-156b1c81a5be",
   "metadata": {},
   "source": [
    "## Sink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec95c249-6eeb-49a5-ae56-53c6910e20d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_SERVER) \\\n",
    "    .option(\"subscribe\", PREDICT_TOPIC) \\\n",
    "    .option(\"failOnDataLoss\", \"false\") \\\n",
    "    .load() \\\n",
    "    .writeStream \\\n",
    "    .foreachBatch(foreach_batch_func) \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/spark/mllib-predict\") \\\n",
    "    .trigger(processingTime=INTERVAL) \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b62552-5398-48a7-ae97-852f095f7b87",
   "metadata": {},
   "source": [
    "# End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc721391-1f82-4344-bd48-e672569b95fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a84f92-467a-4e3a-bf9f-f734226abc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "context.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
