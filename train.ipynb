{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1d44d4c-aea3-45b2-80df-036345822d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cc0bd3-6ef0-448c-b8ec-32d4637637e9",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf3b71c9-a0d3-4a3b-ac8b-27326a78b64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_FILE = \"dataset/dataset.csv\"\n",
    "MODEL_PATH = \"model/\"\n",
    "SPARK_MASTER = \"spark://localhost:5000\"\n",
    "SPARK_APP_NAME = \"Final - PSPD - Train\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8451eed2-b8ea-4558-9216-abb7035d7559",
   "metadata": {},
   "source": [
    "## Startup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b93a72a8-02d4-4265-8ed9-a54b096d558e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/17 21:55:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "conf = SparkConf() \\\n",
    "    .setMaster(SPARK_MASTER) \\\n",
    "    .setAppName(SPARK_APP_NAME)\n",
    "\n",
    "context = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2680644e-d246-47b9-8972-2e02e2d46806",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc862f0-896c-4343-8209-8e42a9151170",
   "metadata": {},
   "source": [
    "## Ingest Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e00f7fee-29b3-4963-ac3d-a331e6bcf4ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "training = spark \\\n",
    "    .read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"sep\", \",\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .load(TRAINING_FILE) \\\n",
    "    .selectExpr(\"sentence\", \"CAST(sentiment AS FLOAT) AS label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c35fc621-ca63-43c2-babb-e213ce3010fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[sentence: string, label: float]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239ace1d-bf9c-4294-9cd8-69217cd9faff",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a0b6a89-c47a-4942-a5f6-3fdcd3b46721",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "lr = LogisticRegression()\n",
    "lrparamGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, [0.001, 0.01, 0.1, 0.5, 1.0, 2.0])\n",
    "             .addGrid(lr.elasticNetParam, [0.0, 0.25, 0.5, 0.75, 1.0])\n",
    "             .addGrid(lr.maxIter, [1, 5, 10, 20])\n",
    "             .build())\n",
    "lrevaluator = RegressionEvaluator(metricName=\"rmse\")\n",
    "lrcv = CrossValidator(estimator = lr,\n",
    "                    estimatorParamMaps = lrparamGrid,\n",
    "                    evaluator = lrevaluator,\n",
    "                    numFolds = 5)\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, lrcv])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906bef28-05cf-4995-a1bf-00f8076a7cc9",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59277966-0680-496b-8900-732d96b859c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/17 21:55:58 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "22/09/17 21:55:58 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n",
      "22/09/17 21:59:34 WARN BlockManager: Asked to remove block broadcast_2712_piece0, which does not exist\n",
      "22/09/17 22:01:36 WARN BlockManager: Asked to remove block broadcast_4514, which does not exist\n",
      "22/09/17 22:02:20 WARN BlockManagerMaster: Failed to remove broadcast 5070 with removeFromMaster = true - Block broadcast_5070 does not exist\n",
      "org.apache.spark.SparkException: Block broadcast_5070 does not exist\n",
      "\tat org.apache.spark.errors.SparkCoreErrors$.blockDoesNotExistError(SparkCoreErrors.scala:234)\n",
      "\tat org.apache.spark.storage.BlockInfoManager.blockInfo(BlockInfoManager.scala:237)\n",
      "\tat org.apache.spark.storage.BlockInfoManager.removeBlock(BlockInfoManager.scala:500)\n",
      "\tat org.apache.spark.storage.BlockManager.removeBlockInternal(BlockManager.scala:1984)\n",
      "\tat org.apache.spark.storage.BlockManager.removeBlock(BlockManager.scala:1962)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$removeBroadcast$3(BlockManager.scala:1948)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$removeBroadcast$3$adapted(BlockManager.scala:1948)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.storage.BlockManager.removeBroadcast(BlockManager.scala:1948)\n",
      "\tat org.apache.spark.storage.BlockManagerStorageEndpoint$$anonfun$receiveAndReply$1.$anonfun$applyOrElse$4(BlockManagerStorageEndpoint.scala:69)\n",
      "\tat scala.runtime.java8.JFunction0$mcI$sp.apply(JFunction0$mcI$sp.java:23)\n",
      "\tat org.apache.spark.storage.BlockManagerStorageEndpoint.$anonfun$doAsync$1(BlockManagerStorageEndpoint.scala:89)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/09/17 22:02:20 ERROR ContextCleaner: Error cleaning broadcast 5070\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.removeBroadcast(BlockManagerMaster.scala:195)\n",
      "\tat org.apache.spark.broadcast.TorrentBroadcast$.unpersist(TorrentBroadcast.scala:351)\n",
      "\tat org.apache.spark.broadcast.TorrentBroadcastFactory.unbroadcast(TorrentBroadcastFactory.scala:45)\n",
      "\tat org.apache.spark.broadcast.BroadcastManager.unbroadcast(BroadcastManager.scala:79)\n",
      "\tat org.apache.spark.ContextCleaner.doCleanupBroadcast(ContextCleaner.scala:256)\n",
      "\tat org.apache.spark.ContextCleaner.$anonfun$keepCleaning$3(ContextCleaner.scala:204)\n",
      "\tat org.apache.spark.ContextCleaner.$anonfun$keepCleaning$3$adapted(ContextCleaner.scala:195)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.ContextCleaner.$anonfun$keepCleaning$1(ContextCleaner.scala:195)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1446)\n",
      "\tat org.apache.spark.ContextCleaner.org$apache$spark$ContextCleaner$$keepCleaning(ContextCleaner.scala:189)\n",
      "\tat org.apache.spark.ContextCleaner$$anon$1.run(ContextCleaner.scala:79)\n",
      "Caused by: org.apache.spark.SparkException: Block broadcast_5070 does not exist\n",
      "\tat org.apache.spark.errors.SparkCoreErrors$.blockDoesNotExistError(SparkCoreErrors.scala:234)\n",
      "\tat org.apache.spark.storage.BlockInfoManager.blockInfo(BlockInfoManager.scala:237)\n",
      "\tat org.apache.spark.storage.BlockInfoManager.removeBlock(BlockInfoManager.scala:500)\n",
      "\tat org.apache.spark.storage.BlockManager.removeBlockInternal(BlockManager.scala:1984)\n",
      "\tat org.apache.spark.storage.BlockManager.removeBlock(BlockManager.scala:1962)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$removeBroadcast$3(BlockManager.scala:1948)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$removeBroadcast$3$adapted(BlockManager.scala:1948)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.storage.BlockManager.removeBroadcast(BlockManager.scala:1948)\n",
      "\tat org.apache.spark.storage.BlockManagerStorageEndpoint$$anonfun$receiveAndReply$1.$anonfun$applyOrElse$4(BlockManagerStorageEndpoint.scala:69)\n",
      "\tat scala.runtime.java8.JFunction0$mcI$sp.apply(JFunction0$mcI$sp.java:23)\n",
      "\tat org.apache.spark.storage.BlockManagerStorageEndpoint.$anonfun$doAsync$1(BlockManagerStorageEndpoint.scala:89)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/09/17 22:03:08 WARN BlockManager: Asked to remove block broadcast_5660_piece0, which does not exist\n"
     ]
    }
   ],
   "source": [
    "model = pipeline.fit(training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f75d67a-a3e7-4863-8cde-b98b1fc127cb",
   "metadata": {},
   "source": [
    "## Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35e90390-66f7-4334-8112-b742e0dc5db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "model.save(F\"{MODEL_PATH}{datetime.now()}.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177dcf02-e47f-46d6-a3b4-f857531fca57",
   "metadata": {},
   "source": [
    "## END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fab73d54-256f-444f-8b30-e4919e93b6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "context.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
