{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82bbc767-43e8-47be-b4cd-bf8bbb7c73b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import length, explode, split, substring, upper, window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "473085c0-1297-47c1-a9cd-5d2865070485",
   "metadata": {},
   "outputs": [],
   "source": [
    "INTERVAL = '3 seconds'\n",
    "SPARK_MASTER = \"spark://localhost:5000\" if True else \"local\"\n",
    "SPARK_APP_NAME = \"Final - PSPD\"\n",
    "KAFKA_SERVER = 'localhost:9093'\n",
    "WORDS_TOPIC = 'wc'\n",
    "STATS_TOPIC = 'statistics'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f0550fa9-43ff-4ded-abba-da8a6f9ae938",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf() \\\n",
    "    .setMaster(SPARK_MASTER) \\\n",
    "    .setAppName(SPARK_APP_NAME) \\\n",
    "    .set(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.0\")\n",
    "    \n",
    "context = SparkContext(conf=conf)\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2f9f9853-aecf-449d-a03e-e4a64629801d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_SERVER) \\\n",
    "    .option(\"subscribe\", WORDS_TOPIC) \\\n",
    "    .option('includeTimestamp', 'true') \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4ad73dc8-8e4a-4f08-98e0-3a0dcb5dbc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the lines into words\n",
    "words = lines.select(\n",
    "    explode(\n",
    "        split(lines.value, \"\\s+\")).alias(\"word\"),\n",
    "        lines.timestamp\n",
    "    )\n",
    "words = words.select(upper(words.word).alias('word'), words.timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c0e22eb8-d4ba-4ff4-836c-c0410ab6b940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group words\n",
    "wordCounts = words.groupBy(\"word\").count()\n",
    "\n",
    "# Count the total of words readed\n",
    "total = words \\\n",
    "    .groupBy() \\\n",
    "    .count() \\\n",
    "    .selectExpr(\"'TOTAL' as key\", \"CAST(count AS STRING) as value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8d853cb6-671a-4846-80b1-31708f14f242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the words that startswith S, P and R\n",
    "letters = words \\\n",
    "    .filter(upper(substring(words.word, 0, 1)).isin([\"S\", \"P\", \"R\"])) \\\n",
    "    .withWatermark(\"timestamp\", INTERVAL) \\\n",
    "    .groupBy(\n",
    "        window(words.timestamp, INTERVAL, INTERVAL),\n",
    "        upper(substring(words.word, 0, 1)).alias(\"key\"),\n",
    "    ) \\\n",
    "    .count() \\\n",
    "    .selectExpr(\"key\", \"CAST(count AS STRING) as value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2b7c8ee7-3368-4bd2-998f-72f1c12eb983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the words that has length 6, 8 and 11\n",
    "lengths = words \\\n",
    "    .filter(length(words.word).isin([6, 8, 11])) \\\n",
    "    .withWatermark(\"timestamp\", INTERVAL) \\\n",
    "    .groupBy(\n",
    "        window(words.timestamp, INTERVAL, INTERVAL),\n",
    "        length(words.word).alias(\"key\")\n",
    "    ) \\\n",
    "    .count() \\\n",
    "    .selectExpr(\"CAST(key AS STRING)\", \"CAST(count AS STRING) as value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "efcc05f7-dee6-4335-aaa9-048882b0aa6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7f349d03-466b-458b-8e92-5963760bd51a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/15 21:48:08 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-9625a6fd-5169-4657-9ac0-4b776a0df835. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "22/09/15 21:48:08 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:>                                                        (0 + 0) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/15 21:48:14 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:>  (0 + 0) / 200][Stage 4:>    (0 + 0) / 1][Stage 5:>    (0 + 0) / 1]1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/15 21:48:29 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/09/15 21:48:44 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/09/15 21:48:59 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/09/15 21:49:14 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:>  (0 + 0) / 200][Stage 4:>    (0 + 0) / 1][Stage 5:>    (0 + 0) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/15 21:49:29 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/09/15 21:49:44 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/09/15 21:49:59 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/09/15 21:50:14 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:>  (0 + 0) / 200][Stage 4:>    (0 + 0) / 1][Stage 5:>    (0 + 0) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/15 21:50:29 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/09/15 21:50:44 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+----+-----+\n",
      "|word|count|\n",
      "+----+-----+\n",
      "+----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/15 21:51:11 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 3000 milliseconds, but spent 173991 milliseconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/15 21:51:17 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 3000 milliseconds, but spent 179043 milliseconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+------+-----+\n",
      "|  word|count|\n",
      "+------+-----+\n",
      "|  PPPP|    1|\n",
      "|   OLA|    1|\n",
      "| MUNDO|    1|\n",
      "| KKKKK|    2|\n",
      "|RRRRRR|    1|\n",
      "| SSSSS|    1|\n",
      "|  AAAA|    1|\n",
      "+------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/15 21:51:26 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 3000 milliseconds, but spent 14999 milliseconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 15:===========>  (167 + 4) / 200][Stage 17:>               (0 + 0) / 200]\r"
     ]
    }
   ],
   "source": [
    "qW = wordCounts \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d0e839bd-8f9a-4604-a55c-86580667d079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/15 21:51:30 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "22/09/15 21:51:30 WARN StreamingQueryManager: Stopping existing streaming query [id=4332b4ba-a805-40e3-b714-25c39cb7b2a5, runId=9db3a083-5b7e-4878-9983-61d6e93897bb], as a new run is being started.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/15 21:51:31 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 3000 milliseconds, but spent 14360 milliseconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:======>                                                (25 + 4) / 200]\r"
     ]
    }
   ],
   "source": [
    "qT = total \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_SERVER) \\\n",
    "    .option('topic', STATS_TOPIC) \\\n",
    "    .option('checkpointLocation', '/tmp/spark/total-stats') \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9fcf6eb3-b186-45b4-8b35-fdb04fea2ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/15 21:51:32 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "22/09/15 21:51:32 WARN StreamingQueryManager: Stopping existing streaming query [id=b7bd206a-2793-401e-a716-7adc5a94f5c5, runId=682b95f4-9987-47cd-b6ac-dc9c94735a89], as a new run is being started.\n",
      "22/09/15 21:51:32 ERROR WriteToDataSourceV2Exec: Data source write support org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite@9b34d46 is aborting.\n",
      "22/09/15 21:51:32 ERROR WriteToDataSourceV2Exec: Data source write support org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite@9b34d46 aborted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:=====>         (74 + 4) / 200][Stage 21:>               (0 + 0) / 200]\r"
     ]
    }
   ],
   "source": [
    "qLen = lengths \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_SERVER) \\\n",
    "    .option('topic', STATS_TOPIC) \\\n",
    "    .option('checkpointLocation', '/tmp/spark/len-stats') \\\n",
    "    .trigger(processingTime=INTERVAL) \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c0bd3945-4f06-40e7-a054-7b1b4acfb7b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/15 21:51:34 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "22/09/15 21:51:34 WARN StreamingQueryManager: Stopping existing streaming query [id=4600f568-b579-4317-b07e-9e492273573e, runId=140e9069-473c-4269-8fce-5089e25ce127], as a new run is being started.\n",
      "22/09/15 21:51:34 ERROR WriteToDataSourceV2Exec: Data source write support org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite@7ec54ee0 is aborting.\n",
      "22/09/15 21:51:34 ERROR WriteToDataSourceV2Exec: Data source write support org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite@7ec54ee0 aborted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:======>        (87 + 4) / 200][Stage 21:>               (0 + 0) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/15 21:51:34 WARN TaskSetManager: Lost task 87.0 in stage 17.0 (TID 1294) (127.0.0.1 executor 156): TaskKilled (Stage cancelled)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 21:>                                                       (0 + 4) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/15 21:51:34 WARN TaskSetManager: Lost task 90.0 in stage 17.0 (TID 1297) (127.0.0.1 executor 156): TaskKilled (Stage cancelled)\n",
      "22/09/15 21:51:34 WARN TaskSetManager: Lost task 89.0 in stage 17.0 (TID 1296) (127.0.0.1 executor 156): TaskKilled (Stage cancelled)\n",
      "22/09/15 21:51:34 WARN TaskSetManager: Lost task 88.0 in stage 17.0 (TID 1295) (127.0.0.1 executor 156): TaskKilled (Stage cancelled)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/15 21:51:38 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 3000 milliseconds, but spent 6067 milliseconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/15 21:51:42 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 3000 milliseconds, but spent 8002 milliseconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+------+-----+\n",
      "|  word|count|\n",
      "+------+-----+\n",
      "|  PPPP|    2|\n",
      "|   OLA|    1|\n",
      "| MUNDO|    1|\n",
      "| KKKKK|    2|\n",
      "|RRRRRR|    1|\n",
      "| SSSSS|    1|\n",
      "|  AAAA|    2|\n",
      "+------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/15 21:51:50 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 3000 milliseconds, but spent 8649 milliseconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/15 21:51:54 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 3000 milliseconds, but spent 12178 milliseconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+------+-----+\n",
      "|  word|count|\n",
      "+------+-----+\n",
      "|  PPPP|    2|\n",
      "|   OLA|    1|\n",
      "| MUNDO|    1|\n",
      "| KKKKK|    3|\n",
      "|RRRRRR|    1|\n",
      "| SSSSS|    1|\n",
      "|  AAAA|    2|\n",
      "|  RRRR|    1|\n",
      "+------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/15 21:52:01 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 3000 milliseconds, but spent 10781 milliseconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/15 21:52:05 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 3000 milliseconds, but spent 10806 milliseconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/15 21:52:08 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 3000 milliseconds, but spent 3362 milliseconds\n"
     ]
    }
   ],
   "source": [
    "qLet = letters \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_SERVER) \\\n",
    "    .option('topic', STATS_TOPIC) \\\n",
    "    .option('checkpointLocation', '/tmp/spark/let-stats') \\\n",
    "    .trigger(processingTime=INTERVAL) \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "906f3d37-3a23-4ca2-a388-936511a978cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "qW.stop()\n",
    "qT.stop()\n",
    "qLen.stop()\n",
    "qLet.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "38479f0c-7da7-4faa-8aba-f7917bdae366",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "context.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
